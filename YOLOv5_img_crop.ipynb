{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6459e0bcee449b090fc9807672725bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c341e1d3bf3b40d1821ce392eb966c68",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_660afee173694231a6dce3cd94df6cae",
              "IPY_MODEL_261218485cef48df961519dde5edfcbe"
            ]
          }
        },
        "c341e1d3bf3b40d1821ce392eb966c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "660afee173694231a6dce3cd94df6cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_32736d503c06497abfae8c0421918255",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 22091032,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 22091032,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e257738711f54d5280c8393d9d3dce1c"
          }
        },
        "261218485cef48df961519dde5edfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_beb7a6fe34b840899bb79c062681696f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21.1M/21.1M [00:00&lt;00:00, 33.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e639132395d64d70b99d8b72c32f8fbb"
          }
        },
        "32736d503c06497abfae8c0421918255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e257738711f54d5280c8393d9d3dce1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "beb7a6fe34b840899bb79c062681696f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e639132395d64d70b99d8b72c32f8fbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smart-able/AI_CV-basic/blob/main/YOLOv5_img_crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvhYZrIZCEyo"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/26833433/98702494-b71c4e80-237a-11eb-87ed-17fcd6b3f066.jpg\">\n",
        "\n",
        "This notebook was written by Ultralytics LLC, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
        "For more information please visit https://github.com/ultralytics/yolov5 and https://www.ultralytics.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b0267a-ff39-4f58-883f-47b4c15b1d4d"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo \n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.12.1+cu113 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Inference\n",
        "\n",
        "`detect.py` runs inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VuZqaxTAnel",
        "outputId": "138fe5d5-139f-4c8b-ee80-4a874a42cb54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -l ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 18932\n",
            "-rw-r--r-- 1 root root 19373732 Oct  1 04:20 20201009.mp4\n",
            "drwxr-xr-x 1 root root     4096 Sep 26 13:45 sample_data\n",
            "drwxr-xr-x 9 root root     4096 Oct  1 04:19 yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "id": "-CcVwEXcxpKB",
        "outputId": "5f39cf9e-9296-48f5-caf0-c5e23f13917f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.6 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "LjmrJFsLxos7",
        "outputId": "fc5fc1cc-1bd7-44d5-e834-270ab206ae1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --source ../20201009.mp4 #동영상이 318개의 프레임으로 구성 "
      ],
      "metadata": {
        "id": "FMRbNk_Px9Cp",
        "outputId": "ab4c28a9-af00-487c-baef-c0273c4fe9fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=yolov5s.pt, source=../20201009.mp4, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v6.2-180-g82bec4c Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 219MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "video 1/1 (1/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 16.6ms\n",
            "video 1/1 (2/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.1ms\n",
            "video 1/1 (3/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 10.9ms\n",
            "video 1/1 (4/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.1ms\n",
            "video 1/1 (5/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 12.0ms\n",
            "video 1/1 (6/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 10.7ms\n",
            "video 1/1 (7/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (8/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.5ms\n",
            "video 1/1 (9/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (10/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 12.3ms\n",
            "video 1/1 (11/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.1ms\n",
            "video 1/1 (12/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.5ms\n",
            "video 1/1 (13/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.5ms\n",
            "video 1/1 (14/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 21.8ms\n",
            "video 1/1 (15/318) /content/20201009.mp4: 384x640 2 persons, 2 bicycles, 1 backpack, 16.2ms\n",
            "video 1/1 (16/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 backpack, 12.1ms\n",
            "video 1/1 (17/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.0ms\n",
            "video 1/1 (18/318) /content/20201009.mp4: 384x640 2 persons, 2 bicycles, 10.9ms\n",
            "video 1/1 (19/318) /content/20201009.mp4: 384x640 5 persons, 2 bicycles, 11.4ms\n",
            "video 1/1 (20/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 11.1ms\n",
            "video 1/1 (21/318) /content/20201009.mp4: 384x640 2 persons, 3 bicycles, 12.3ms\n",
            "video 1/1 (22/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 12.8ms\n",
            "video 1/1 (23/318) /content/20201009.mp4: 384x640 5 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (24/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (25/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.0ms\n",
            "video 1/1 (26/318) /content/20201009.mp4: 384x640 5 persons, 2 bicycles, 10.8ms\n",
            "video 1/1 (27/318) /content/20201009.mp4: 384x640 5 persons, 2 bicycles, 11.0ms\n",
            "video 1/1 (28/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 12.4ms\n",
            "video 1/1 (29/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.0ms\n",
            "video 1/1 (30/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 10.4ms\n",
            "video 1/1 (31/318) /content/20201009.mp4: 384x640 2 persons, 3 bicycles, 14.8ms\n",
            "video 1/1 (32/318) /content/20201009.mp4: 384x640 2 persons, 2 bicycles, 11.0ms\n",
            "video 1/1 (33/318) /content/20201009.mp4: 384x640 2 persons, 2 bicycles, 11.1ms\n",
            "video 1/1 (34/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.0ms\n",
            "video 1/1 (35/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 12.0ms\n",
            "video 1/1 (36/318) /content/20201009.mp4: 384x640 2 persons, 3 bicycles, 12.4ms\n",
            "video 1/1 (37/318) /content/20201009.mp4: 384x640 2 persons, 3 bicycles, 11.7ms\n",
            "video 1/1 (38/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 12.1ms\n",
            "video 1/1 (39/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 12.2ms\n",
            "video 1/1 (40/318) /content/20201009.mp4: 384x640 2 persons, 3 bicycles, 1 backpack, 11.2ms\n",
            "video 1/1 (41/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.5ms\n",
            "video 1/1 (42/318) /content/20201009.mp4: 384x640 2 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (43/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (44/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 11.1ms\n",
            "video 1/1 (45/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.7ms\n",
            "video 1/1 (46/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (47/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.7ms\n",
            "video 1/1 (48/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 backpack, 11.4ms\n",
            "video 1/1 (49/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.4ms\n",
            "video 1/1 (50/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 backpack, 10.8ms\n",
            "video 1/1 (51/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (52/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.4ms\n",
            "video 1/1 (53/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.3ms\n",
            "video 1/1 (54/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.4ms\n",
            "video 1/1 (55/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 1 motorcycle, 12.3ms\n",
            "video 1/1 (56/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 1 motorcycle, 1 traffic light, 11.4ms\n",
            "video 1/1 (57/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 motorcycle, 10.9ms\n",
            "video 1/1 (58/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 motorcycle, 1 traffic light, 11.1ms\n",
            "video 1/1 (59/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 1 motorcycle, 11.7ms\n",
            "video 1/1 (60/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 1 motorcycle, 10.9ms\n",
            "video 1/1 (61/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 motorcycle, 11.1ms\n",
            "video 1/1 (62/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 13.4ms\n",
            "video 1/1 (63/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 1 motorcycle, 11.4ms\n",
            "video 1/1 (64/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 1 motorcycle, 11.1ms\n",
            "video 1/1 (65/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 1 motorcycle, 12.3ms\n",
            "video 1/1 (66/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (67/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.8ms\n",
            "video 1/1 (68/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.8ms\n",
            "video 1/1 (69/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.4ms\n",
            "video 1/1 (70/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (71/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.7ms\n",
            "video 1/1 (72/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 12.3ms\n",
            "video 1/1 (73/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (74/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.7ms\n",
            "video 1/1 (75/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (76/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.8ms\n",
            "video 1/1 (77/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.5ms\n",
            "video 1/1 (78/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.2ms\n",
            "video 1/1 (79/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 12.5ms\n",
            "video 1/1 (80/318) /content/20201009.mp4: 384x640 5 persons, 2 bicycles, 11.2ms\n",
            "video 1/1 (81/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.9ms\n",
            "video 1/1 (82/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 skateboard, 11.5ms\n",
            "video 1/1 (83/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.4ms\n",
            "video 1/1 (84/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.3ms\n",
            "video 1/1 (85/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.1ms\n",
            "video 1/1 (86/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 18.0ms\n",
            "video 1/1 (87/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.6ms\n",
            "video 1/1 (88/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 13.1ms\n",
            "video 1/1 (89/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 11.4ms\n",
            "video 1/1 (90/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 10.6ms\n",
            "video 1/1 (91/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (92/318) /content/20201009.mp4: 384x640 3 persons, 3 bicycles, 1 traffic light, 11.2ms\n",
            "video 1/1 (93/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 1 skateboard, 11.5ms\n",
            "video 1/1 (94/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.0ms\n",
            "video 1/1 (95/318) /content/20201009.mp4: 384x640 4 persons, 1 bicycle, 1 traffic light, 11.4ms\n",
            "video 1/1 (96/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (97/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.3ms\n",
            "video 1/1 (98/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (99/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.8ms\n",
            "video 1/1 (100/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.9ms\n",
            "video 1/1 (101/318) /content/20201009.mp4: 384x640 4 persons, 4 bicycles, 11.1ms\n",
            "video 1/1 (102/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.7ms\n",
            "video 1/1 (103/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 1 motorcycle, 11.1ms\n",
            "video 1/1 (104/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.3ms\n",
            "video 1/1 (105/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.5ms\n",
            "video 1/1 (106/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.1ms\n",
            "video 1/1 (107/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.2ms\n",
            "video 1/1 (108/318) /content/20201009.mp4: 384x640 4 persons, 2 bicycles, 11.2ms\n",
            "video 1/1 (109/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.6ms\n",
            "video 1/1 (110/318) /content/20201009.mp4: 384x640 4 persons, 3 bicycles, 12.2ms\n",
            "video 1/1 (111/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 traffic light, 11.5ms\n",
            "video 1/1 (112/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 traffic light, 12.9ms\n",
            "video 1/1 (113/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.2ms\n",
            "video 1/1 (114/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.2ms\n",
            "video 1/1 (115/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.1ms\n",
            "video 1/1 (116/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.3ms\n",
            "video 1/1 (117/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.1ms\n",
            "video 1/1 (118/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 traffic light, 10.8ms\n",
            "video 1/1 (119/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 1 traffic light, 11.3ms\n",
            "video 1/1 (120/318) /content/20201009.mp4: 384x640 3 persons, 2 bicycles, 11.2ms\n",
            "video 1/1 (121/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.4ms\n",
            "video 1/1 (122/318) /content/20201009.mp4: 384x640 4 persons, 1 bicycle, 11.0ms\n",
            "video 1/1 (123/318) /content/20201009.mp4: 384x640 4 persons, 11.3ms\n",
            "video 1/1 (124/318) /content/20201009.mp4: 384x640 4 persons, 12.0ms\n",
            "video 1/1 (125/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (126/318) /content/20201009.mp4: 384x640 4 persons, 1 bicycle, 11.4ms\n",
            "video 1/1 (127/318) /content/20201009.mp4: 384x640 4 persons, 27.5ms\n",
            "video 1/1 (128/318) /content/20201009.mp4: 384x640 6 persons, 11.3ms\n",
            "video 1/1 (129/318) /content/20201009.mp4: 384x640 6 persons, 11.0ms\n",
            "video 1/1 (130/318) /content/20201009.mp4: 384x640 4 persons, 11.0ms\n",
            "video 1/1 (131/318) /content/20201009.mp4: 384x640 5 persons, 11.1ms\n",
            "video 1/1 (132/318) /content/20201009.mp4: 384x640 4 persons, 11.3ms\n",
            "video 1/1 (133/318) /content/20201009.mp4: 384x640 5 persons, 11.1ms\n",
            "video 1/1 (134/318) /content/20201009.mp4: 384x640 6 persons, 11.3ms\n",
            "video 1/1 (135/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (136/318) /content/20201009.mp4: 384x640 4 persons, 11.2ms\n",
            "video 1/1 (137/318) /content/20201009.mp4: 384x640 4 persons, 11.9ms\n",
            "video 1/1 (138/318) /content/20201009.mp4: 384x640 4 persons, 11.2ms\n",
            "video 1/1 (139/318) /content/20201009.mp4: 384x640 4 persons, 12.4ms\n",
            "video 1/1 (140/318) /content/20201009.mp4: 384x640 4 persons, 11.3ms\n",
            "video 1/1 (141/318) /content/20201009.mp4: 384x640 4 persons, 11.0ms\n",
            "video 1/1 (142/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (143/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (144/318) /content/20201009.mp4: 384x640 3 persons, 1 car, 16.5ms\n",
            "video 1/1 (145/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.3ms\n",
            "video 1/1 (146/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.4ms\n",
            "video 1/1 (147/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 1 car, 11.3ms\n",
            "video 1/1 (148/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 11.2ms\n",
            "video 1/1 (149/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (150/318) /content/20201009.mp4: 384x640 3 persons, 10.6ms\n",
            "video 1/1 (151/318) /content/20201009.mp4: 384x640 3 persons, 15.9ms\n",
            "video 1/1 (152/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (153/318) /content/20201009.mp4: 384x640 3 persons, 12.1ms\n",
            "video 1/1 (154/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (155/318) /content/20201009.mp4: 384x640 3 persons, 10.6ms\n",
            "video 1/1 (156/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (157/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (158/318) /content/20201009.mp4: 384x640 3 persons, 16.0ms\n",
            "video 1/1 (159/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (160/318) /content/20201009.mp4: 384x640 3 persons, 10.8ms\n",
            "video 1/1 (161/318) /content/20201009.mp4: 384x640 4 persons, 12.2ms\n",
            "video 1/1 (162/318) /content/20201009.mp4: 384x640 4 persons, 11.2ms\n",
            "video 1/1 (163/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (164/318) /content/20201009.mp4: 384x640 3 persons, 10.5ms\n",
            "video 1/1 (165/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (166/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (167/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (168/318) /content/20201009.mp4: 384x640 3 persons, 13.1ms\n",
            "video 1/1 (169/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (170/318) /content/20201009.mp4: 384x640 3 persons, 13.7ms\n",
            "video 1/1 (171/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (172/318) /content/20201009.mp4: 384x640 3 persons, 11.9ms\n",
            "video 1/1 (173/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (174/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (175/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (176/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (177/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (178/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (179/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (180/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 10.8ms\n",
            "video 1/1 (181/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 13.2ms\n",
            "video 1/1 (182/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (183/318) /content/20201009.mp4: 384x640 3 persons, 10.9ms\n",
            "video 1/1 (184/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (185/318) /content/20201009.mp4: 384x640 3 persons, 10.9ms\n",
            "video 1/1 (186/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 12.1ms\n",
            "video 1/1 (187/318) /content/20201009.mp4: 384x640 3 persons, 1 bicycle, 13.6ms\n",
            "video 1/1 (188/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (189/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (190/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (191/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (192/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (193/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (194/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (195/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (196/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (197/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (198/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (199/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (200/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (201/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (202/318) /content/20201009.mp4: 384x640 4 persons, 11.1ms\n",
            "video 1/1 (203/318) /content/20201009.mp4: 384x640 4 persons, 11.0ms\n",
            "video 1/1 (204/318) /content/20201009.mp4: 384x640 4 persons, 12.2ms\n",
            "video 1/1 (205/318) /content/20201009.mp4: 384x640 4 persons, 18.3ms\n",
            "video 1/1 (206/318) /content/20201009.mp4: 384x640 3 persons, 13.3ms\n",
            "video 1/1 (207/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (208/318) /content/20201009.mp4: 384x640 4 persons, 11.2ms\n",
            "video 1/1 (209/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (210/318) /content/20201009.mp4: 384x640 4 persons, 10.7ms\n",
            "video 1/1 (211/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (212/318) /content/20201009.mp4: 384x640 3 persons, 12.1ms\n",
            "video 1/1 (213/318) /content/20201009.mp4: 384x640 3 persons, 12.4ms\n",
            "video 1/1 (214/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (215/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (216/318) /content/20201009.mp4: 384x640 3 persons, 10.8ms\n",
            "video 1/1 (217/318) /content/20201009.mp4: 384x640 3 persons, 10.4ms\n",
            "video 1/1 (218/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (219/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (220/318) /content/20201009.mp4: 384x640 3 persons, 10.3ms\n",
            "video 1/1 (221/318) /content/20201009.mp4: 384x640 3 persons, 11.7ms\n",
            "video 1/1 (222/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (223/318) /content/20201009.mp4: 384x640 3 persons, 10.8ms\n",
            "video 1/1 (224/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (225/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (226/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (227/318) /content/20201009.mp4: 384x640 3 persons, 12.1ms\n",
            "video 1/1 (228/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (229/318) /content/20201009.mp4: 384x640 3 persons, 14.4ms\n",
            "video 1/1 (230/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (231/318) /content/20201009.mp4: 384x640 3 persons, 12.7ms\n",
            "video 1/1 (232/318) /content/20201009.mp4: 384x640 3 persons, 12.8ms\n",
            "video 1/1 (233/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (234/318) /content/20201009.mp4: 384x640 3 persons, 13.5ms\n",
            "video 1/1 (235/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (236/318) /content/20201009.mp4: 384x640 2 persons, 11.0ms\n",
            "video 1/1 (237/318) /content/20201009.mp4: 384x640 3 persons, 12.1ms\n",
            "video 1/1 (238/318) /content/20201009.mp4: 384x640 2 persons, 12.2ms\n",
            "video 1/1 (239/318) /content/20201009.mp4: 384x640 2 persons, 13.7ms\n",
            "video 1/1 (240/318) /content/20201009.mp4: 384x640 2 persons, 10.8ms\n",
            "video 1/1 (241/318) /content/20201009.mp4: 384x640 3 persons, 12.1ms\n",
            "video 1/1 (242/318) /content/20201009.mp4: 384x640 1 person, 11.9ms\n",
            "video 1/1 (243/318) /content/20201009.mp4: 384x640 1 person, 11.5ms\n",
            "video 1/1 (244/318) /content/20201009.mp4: 384x640 1 person, 38.7ms\n",
            "video 1/1 (245/318) /content/20201009.mp4: 384x640 1 person, 21.8ms\n",
            "video 1/1 (246/318) /content/20201009.mp4: 384x640 2 persons, 11.4ms\n",
            "video 1/1 (247/318) /content/20201009.mp4: 384x640 2 persons, 11.3ms\n",
            "video 1/1 (248/318) /content/20201009.mp4: 384x640 1 person, 11.3ms\n",
            "video 1/1 (249/318) /content/20201009.mp4: 384x640 1 person, 11.5ms\n",
            "video 1/1 (250/318) /content/20201009.mp4: 384x640 1 person, 13.3ms\n",
            "video 1/1 (251/318) /content/20201009.mp4: 384x640 1 person, 11.2ms\n",
            "video 1/1 (252/318) /content/20201009.mp4: 384x640 1 person, 11.2ms\n",
            "video 1/1 (253/318) /content/20201009.mp4: 384x640 1 person, 11.6ms\n",
            "video 1/1 (254/318) /content/20201009.mp4: 384x640 1 person, 11.0ms\n",
            "video 1/1 (255/318) /content/20201009.mp4: 384x640 1 person, 11.6ms\n",
            "video 1/1 (256/318) /content/20201009.mp4: 384x640 1 person, 11.4ms\n",
            "video 1/1 (257/318) /content/20201009.mp4: 384x640 1 person, 10.9ms\n",
            "video 1/1 (258/318) /content/20201009.mp4: 384x640 1 person, 11.4ms\n",
            "video 1/1 (259/318) /content/20201009.mp4: 384x640 1 person, 12.1ms\n",
            "video 1/1 (260/318) /content/20201009.mp4: 384x640 1 person, 41.4ms\n",
            "video 1/1 (261/318) /content/20201009.mp4: 384x640 1 person, 11.3ms\n",
            "video 1/1 (262/318) /content/20201009.mp4: 384x640 1 person, 9.6ms\n",
            "video 1/1 (263/318) /content/20201009.mp4: 384x640 1 person, 11.7ms\n",
            "video 1/1 (264/318) /content/20201009.mp4: 384x640 1 person, 12.3ms\n",
            "video 1/1 (265/318) /content/20201009.mp4: 384x640 (no detections), 11.2ms\n",
            "video 1/1 (266/318) /content/20201009.mp4: 384x640 (no detections), 11.7ms\n",
            "video 1/1 (267/318) /content/20201009.mp4: 384x640 2 persons, 11.2ms\n",
            "video 1/1 (268/318) /content/20201009.mp4: 384x640 3 persons, 10.8ms\n",
            "video 1/1 (269/318) /content/20201009.mp4: 384x640 3 persons, 16.2ms\n",
            "video 1/1 (270/318) /content/20201009.mp4: 384x640 3 persons, 10.9ms\n",
            "video 1/1 (271/318) /content/20201009.mp4: 384x640 3 persons, 16.9ms\n",
            "video 1/1 (272/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (273/318) /content/20201009.mp4: 384x640 3 persons, 11.7ms\n",
            "video 1/1 (274/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (275/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (276/318) /content/20201009.mp4: 384x640 3 persons, 12.7ms\n",
            "video 1/1 (277/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (278/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (279/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (280/318) /content/20201009.mp4: 384x640 3 persons, 11.4ms\n",
            "video 1/1 (281/318) /content/20201009.mp4: 384x640 3 persons, 11.7ms\n",
            "video 1/1 (282/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (283/318) /content/20201009.mp4: 384x640 3 persons, 14.0ms\n",
            "video 1/1 (284/318) /content/20201009.mp4: 384x640 3 persons, 11.6ms\n",
            "video 1/1 (285/318) /content/20201009.mp4: 384x640 2 persons, 11.5ms\n",
            "video 1/1 (286/318) /content/20201009.mp4: 384x640 2 persons, 11.9ms\n",
            "video 1/1 (287/318) /content/20201009.mp4: 384x640 3 persons, 11.7ms\n",
            "video 1/1 (288/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (289/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (290/318) /content/20201009.mp4: 384x640 3 persons, 11.1ms\n",
            "video 1/1 (291/318) /content/20201009.mp4: 384x640 3 persons, 12.8ms\n",
            "video 1/1 (292/318) /content/20201009.mp4: 384x640 3 persons, 11.2ms\n",
            "video 1/1 (293/318) /content/20201009.mp4: 384x640 3 persons, 12.0ms\n",
            "video 1/1 (294/318) /content/20201009.mp4: 384x640 3 persons, 11.7ms\n",
            "video 1/1 (295/318) /content/20201009.mp4: 384x640 3 persons, 14.0ms\n",
            "video 1/1 (296/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (297/318) /content/20201009.mp4: 384x640 3 persons, 11.8ms\n",
            "video 1/1 (298/318) /content/20201009.mp4: 384x640 2 persons, 11.4ms\n",
            "video 1/1 (299/318) /content/20201009.mp4: 384x640 2 persons, 12.1ms\n",
            "video 1/1 (300/318) /content/20201009.mp4: 384x640 2 persons, 11.3ms\n",
            "video 1/1 (301/318) /content/20201009.mp4: 384x640 3 persons, 11.5ms\n",
            "video 1/1 (302/318) /content/20201009.mp4: 384x640 3 persons, 11.3ms\n",
            "video 1/1 (303/318) /content/20201009.mp4: 384x640 3 persons, 11.9ms\n",
            "video 1/1 (304/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (305/318) /content/20201009.mp4: 384x640 2 persons, 11.1ms\n",
            "video 1/1 (306/318) /content/20201009.mp4: 384x640 3 persons, 11.0ms\n",
            "video 1/1 (307/318) /content/20201009.mp4: 384x640 2 persons, 11.9ms\n",
            "video 1/1 (308/318) /content/20201009.mp4: 384x640 2 persons, 11.1ms\n",
            "video 1/1 (309/318) /content/20201009.mp4: 384x640 2 persons, 11.5ms\n",
            "video 1/1 (310/318) /content/20201009.mp4: 384x640 2 persons, 12.0ms\n",
            "video 1/1 (311/318) /content/20201009.mp4: 384x640 1 person, 11.1ms\n",
            "video 1/1 (312/318) /content/20201009.mp4: 384x640 1 person, 11.3ms\n",
            "video 1/1 (313/318) /content/20201009.mp4: 384x640 1 person, 11.1ms\n",
            "video 1/1 (314/318) /content/20201009.mp4: 384x640 2 persons, 11.1ms\n",
            "video 1/1 (315/318) /content/20201009.mp4: 384x640 1 person, 11.2ms\n",
            "video 1/1 (316/318) /content/20201009.mp4: 384x640 2 persons, 12.3ms\n",
            "video 1/1 (317/318) /content/20201009.mp4: 384x640 1 person, 11.5ms\n",
            "video 1/1 (318/318) /content/20201009.mp4: 384x640 1 person, 9.1ms\n",
            "Speed: 0.5ms pre-process, 12.0ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --source ../elephant2.png"
      ],
      "metadata": {
        "id": "nMmSSrRWx838",
        "outputId": "6c5a62b1-e358-4296-e9e3-4bf754a84c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=yolov5s.pt, source=../elephant2.png, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v6.2-180-g82bec4c Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "image 1/1 /content/elephant2.png: 384x640 2 elephants, 12.5ms\n",
            "Speed: 0.4ms pre-process, 12.5ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp4\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --source ../traffic-signal-1.jpg"
      ],
      "metadata": {
        "id": "5B8_9vfRx8ZV",
        "outputId": "25a58600-2d1a-41e9-de8c-4800f2a682a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=yolov5s.pt, source=../traffic-signal-1.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v6.2-180-g82bec4c Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "image 1/1 /content/traffic-signal-1.jpg: 640x640 11 cars, 7 traffic lights, 13.6ms\n",
            "Speed: 0.6ms pre-process, 13.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgaLo9B4ZGzx"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qbaa3iEcrcE"
      },
      "source": [
        "Results are saved to `runs/detect`. A full list of available inference sources:\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/98274798-2b7a7a80-1f94-11eb-91a4-70c73593e26b.jpg\" width=\"900\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Test\n",
        "Test a model on [COCO](https://cocodataset.org/#home) val or test-dev dataset to evaluate trained accuracy. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be 1-2% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTZYGgRjnMc"
      },
      "source": [
        "## COCO val2017\n",
        "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_"
      },
      "source": [
        "# Download COCO val2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH"
      },
      "source": [
        "# Run YOLOv5x on COCO val2017\n",
        "!python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_KbFk0juX2"
      },
      "source": [
        "## COCO test-dev2017\n",
        "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (20,000 images). Results are saved to a `*.json` file which can be submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0AJnSeCIHyJ"
      },
      "source": [
        "# Download COCO test-dev2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip  # unzip labels\n",
        "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n",
        "%mv ./test2017 ./coco/images && mv ./coco ../  # move images to /coco and move /coco next to /yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29GJXAP_lPrt"
      },
      "source": [
        "# Run YOLOv5s on COCO test-dev2017 using --task test\n",
        "!python test.py --weights yolov5s.pt --data coco.yaml --task test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knxi2ncxWffW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "e6459e0bcee449b090fc9807672725bc",
            "c341e1d3bf3b40d1821ce392eb966c68",
            "660afee173694231a6dce3cd94df6cae",
            "261218485cef48df961519dde5edfcbe",
            "32736d503c06497abfae8c0421918255",
            "e257738711f54d5280c8393d9d3dce1c",
            "beb7a6fe34b840899bb79c062681696f",
            "e639132395d64d70b99d8b72c32f8fbb"
          ]
        },
        "outputId": "e8b7d5b3-a71e-4446-eec2-ad13419cf700"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6459e0bcee449b090fc9807672725bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=22091032.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOkGLv1dMqh"
      },
      "source": [
        "Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
        "\n",
        "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "source": [
        "# Tensorboard (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLAV42oNb7M"
      },
      "source": [
        "# Weights & Biases (optional)\n",
        "%pip install -q wandb  \n",
        "!wandb login  # use 'wandb disabled' or 'wandb enabled' to disable or enable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e51b29-2df4-4f00-cde8-5f6e4a34da9e"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 v4.0-75-gbdd88e1 torch 1.7.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=16, bucket='', cache_images=True, cfg='', data='./data/coco128.yaml', device='', epochs=3, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], linear_lr=False, local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='exp', noautoanchor=False, nosave=True, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp', single_cls=False, sync_bn=False, total_batch_size=16, weights='yolov5s.pt', workers=8, world_size=1)\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "2021-02-12 06:38:28.027271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:01<00:00, 13.2MB/s]\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n",
            "\n",
            "Transferred 362/362 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../coco128/labels/train2017' for images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 2566.00it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 175.07it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../coco128/labels/train2017.cache' for images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 764773.38it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 128.17it/s]\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.26, Best Possible Recall (BPR) = 0.9946\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       0/2     3.27G   0.04357   0.06781   0.01869    0.1301       207       640: 100% 8/8 [00:03<00:00,  2.03it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:04<00:00,  1.14s/it]\n",
            "                 all         128         929       0.646       0.627       0.659       0.431\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       1/2     7.75G   0.04308   0.06654   0.02083    0.1304       227       640: 100% 8/8 [00:01<00:00,  4.11it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:01<00:00,  2.94it/s]\n",
            "                 all         128         929       0.681       0.607       0.663       0.434\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       2/2     7.75G   0.04461   0.06896   0.01866    0.1322       191       640: 100% 8/8 [00:02<00:00,  3.94it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:03<00:00,  1.22it/s]\n",
            "                 all         128         929       0.642       0.632       0.662       0.432\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.8MB\n",
            "3 epochs completed in 0.007 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging 🌟 NEW\n",
        "\n",
        "[Weights & Biases](https://www.wandb.com/) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPdhraOTCO0"
      },
      "source": [
        "Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n",
        "Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # test batch 0 labels\n",
        "Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # test batch 0 predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYG4WFEnTVrI"
      },
      "source": [
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"750\">  \n",
        "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"750\">  \n",
        "`test_batch0_labels.jpg` shows test batch 0 labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"750\">  \n",
        "`test_batch0_pred.jpg` shows test batch 0 _predictions_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov5s.pt` (orange)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDznIqPF7nk3"
      },
      "source": [
        "from utils.plots import plot_results \n",
        "plot_results(save_dir='runs/train/exp')  # plot all results*.txt as results.png\n",
        "Image(filename='runs/train/exp/results.png', width=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrEegCSW3fK"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/26833433/97808309-8182b180-1c66-11eb-8461-bffe1a79511d.png\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI6NoBev8Ib1"
      },
      "source": [
        "# Re-clone repo\n",
        "%cd ..\n",
        "%rm -rf yolov5 && git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKoSIK2WSzj"
      },
      "source": [
        "# Reproduce\n",
        "%%shell\n",
        "for x in yolov5s yolov5m yolov5l yolov5x; do\n",
        "  python test.py --weights $x.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n",
        "  python test.py --weights $x.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGH0ZjkGjejy"
      },
      "source": [
        "# Unit tests\n",
        "%%shell\n",
        "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
        "\n",
        "rm -rf runs  # remove runs/\n",
        "for m in yolov5s; do  # models\n",
        "  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n",
        "  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n",
        "  for d in 0 cpu; do  # devices\n",
        "    python detect.py --weights $m.pt --device $d  # detect official\n",
        "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
        "    python test.py --weights $m.pt --device $d # test official\n",
        "    python test.py --weights runs/train/exp/weights/best.pt --device $d # test custom\n",
        "  done\n",
        "  python hubconf.py  # hub\n",
        "  python models/yolo.py --cfg $m.yaml  # inspect\n",
        "  python models/export.py --weights $m.pt --img 640 --batch 1  # export\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogI-kwi3Tye"
      },
      "source": [
        "# Profile\n",
        "from utils.torch_utils import profile \n",
        "\n",
        "m1 = lambda x: x * torch.sigmoid(x)\n",
        "m2 = torch.nn.SiLU()\n",
        "profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVRSOhEvUdb5"
      },
      "source": [
        "# Evolve\n",
        "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
        "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSgFCAcMbk1R"
      },
      "source": [
        "# VOC\n",
        "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
        "  !python train.py --batch {b} --weights {m}.pt --data voc.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}